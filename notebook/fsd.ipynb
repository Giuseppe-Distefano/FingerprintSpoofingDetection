{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fingerprint Spoofing Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Giuseppe-Distefano/FingerprintSpoofingDetection\n",
    "!cp -r ./FingerprintSpoofingDetection/data ./\n",
    "!rm -rf ./FingerprintSpoofingDetection\n",
    "\n",
    "!mkdir output\n",
    "!mkdir output/FeaturesAnalysis\n",
    "!mkdir output/FeaturesAnalysis/Histograms output/FeaturesAnalysis/Heatmaps\n",
    "!mkdir output/DimensionalityReduction\n",
    "!mkdir output/DimensionalityReduction/PCA output/DimensionalityReduction/LDA\n",
    "!mkdir output/Training\n",
    "!mkdir output/Calibration_Fusion\n",
    "!mkdir output/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.linalg\n",
    "import numpy.linalg\n",
    "import scipy.special\n",
    "import matplotlib.pyplot\n",
    "import seaborn\n",
    "import scipy.optimize\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_t = 0.5\n",
    "Cfn = 1\n",
    "Cfp = 10\n",
    "effective_prior = (pi_t*Cfn) / (pi_t*Cfn + (1-pi_t)*Cfp)\n",
    "output_csv_name = \"output/Training/Results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Convert an array into a column -----\n",
    "def row_to_column (array):\n",
    "    return array.reshape((array.size, 1))\n",
    "\n",
    "\n",
    "# ----- Convert an array into a row -----\n",
    "def column_to_row (array):\n",
    "    return array.reshape((1, array.size))\n",
    "\n",
    "\n",
    "# ----- Compute mean -----\n",
    "def compute_mean (data):\n",
    "    return row_to_column(data.mean(1))\n",
    "\n",
    "\n",
    "# ----- Compute covariance -----\n",
    "def compute_covariance (data):\n",
    "    mu = compute_mean(data)\n",
    "    dc = data - mu.reshape((mu.size, 1))\n",
    "    cov = numpy.dot(dc, dc.T) / float(data.shape[1])\n",
    "    return cov\n",
    "\n",
    "\n",
    "# ----- Compute Pearson correlation -----\n",
    "def compute_correlation(x, y):\n",
    "    x1_sum, y1_sum = numpy.sum(x), numpy.sum(y)\n",
    "    x2_sum, y2_sum = numpy.sum(x**2), numpy.sum(y**2)\n",
    "    cp_sum = numpy.sum(x * y.T)\n",
    "    n = x.shape[0]\n",
    "\n",
    "    num = n*cp_sum - x1_sum*y1_sum\n",
    "    den = numpy.sqrt((n*x2_sum - x1_sum**2) * (n*y2_sum - y1_sum**2))\n",
    "    return (num/den)\n",
    "\n",
    "\n",
    "# ----- Compute logarithm of probability density function of a Gaussian distribution -----\n",
    "def logpdf_GAU_ND (X, mu, C):\n",
    "    _,det = numpy.linalg.slogdet(C)\n",
    "    inv = numpy.linalg.inv(C)\n",
    "\n",
    "    term1 = -0.5 * X.shape[0] * numpy.log(2*numpy.pi)\n",
    "    term2 = -0.5 * det\n",
    "    term3 = -0.5 * numpy.dot((X-mu).T, numpy.dot(inv, (X-mu))).sum(0)\n",
    "\n",
    "    return (term1+term2+term3)\n",
    "\n",
    "\n",
    "# ----- Classify samples matching log-likelihood ratio and a threshold -----\n",
    "def predict_labels (llr, threshold):\n",
    "    predicted = numpy.zeros(len(llr))\n",
    "    for i in range(len(llr)):\n",
    "        if (llr[i]>threshold): predicted[i] = 1\n",
    "    return predicted\n",
    "\n",
    "\n",
    "# ----- Count the number of mispredictions -----\n",
    "def count_mispredictions (predicted, LTE):\n",
    "    wrong = 0\n",
    "    for i in range(len(LTE)):\n",
    "        if (predicted[i]!=LTE[i]): wrong += 1\n",
    "    return wrong\n",
    "\n",
    "\n",
    "# ----- Square a matrix and transpose it -----\n",
    "def square_and_transpose (matrix):\n",
    "    x = matrix[:,None]\n",
    "    xxT = x.dot(x.T).reshape((x.size)**2, order='F')\n",
    "    return xxT\n",
    "\n",
    "\n",
    "# ----- Compute ZNorm -----\n",
    "def compute_znorm (DTR, DTE):\n",
    "    mu_r = row_to_column(DTR.mean(1))\n",
    "    sigma_r = row_to_column(DTR.std(1))\n",
    "    normalized_r = (DTR-mu_r) / sigma_r\n",
    "    normalized_e = (DTE-mu_r) / sigma_r\n",
    "    return normalized_r, normalized_e\n",
    "\n",
    "\n",
    "# ----- Randomize data -----\n",
    "def randomize (D, L, seed):\n",
    "    numpy.random.seed(seed)\n",
    "    indexes = numpy.random.permutation(D.shape[1])\n",
    "    d = numpy.zeros((10, D.shape[1]))\n",
    "    l = numpy.zeros((L.size,))\n",
    "    i = 0\n",
    "    for ind in indexes:\n",
    "        d[:,i] = D[:,ind]\n",
    "        l[i] = L[ind]\n",
    "        i += 1\n",
    "    return d, l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#     GLOBAL VARIABLES     #\n",
    "############################\n",
    "features = int(10)\n",
    "distinct_classes = int(2)\n",
    "training_input = \"data/Train.txt\"\n",
    "test_input = \"data/Test.txt\"\n",
    "histograms_folder = \"output/FeaturesAnalysis/Histograms\"\n",
    "heatmaps_folder = \"output/FeaturesAnalysis/Heatmaps\"\n",
    "\n",
    "\n",
    "#####################\n",
    "#     FUNCTIONS     #\n",
    "#####################\n",
    "# ----- Read file -----\n",
    "def read_file (filename):\n",
    "    D = []\n",
    "    L = []\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                attributes = line.split(\",\")[0:features]\n",
    "                attributes = row_to_column(numpy.array([float(i) for i in attributes]))\n",
    "                label = int(line.split(\",\")[-1].strip())\n",
    "                D.append(attributes)\n",
    "                L.append(label)\n",
    "            except:\n",
    "                pass\n",
    "    return numpy.hstack(D), numpy.array(L, dtype=numpy.int32)\n",
    "\n",
    "\n",
    "# ----- Load training set -----\n",
    "def load_training_set ():\n",
    "    DTR,LTR = read_file(training_input)\n",
    "    return DTR,LTR\n",
    "\n",
    "\n",
    "# ----- Load test set -----\n",
    "def load_test_set ():\n",
    "    DTE,LTE = read_file(test_input)\n",
    "    return DTE,LTE\n",
    "\n",
    "\n",
    "# ----- Plot histograms of training set -----\n",
    "def plot_histograms (DTR, LTR):\n",
    "    # Split according to label\n",
    "    D0 = DTR[:, LTR==0]\n",
    "    D1 = DTR[:, LTR==1]\n",
    "\n",
    "    # Plot histograms for each feature\n",
    "    for x in range(features):\n",
    "        matplotlib.pyplot.figure()\n",
    "        matplotlib.pyplot.title(\"Feature %d\" % x)\n",
    "        matplotlib.pyplot.hist(D0[x,:], bins=40, density=True, alpha=0.4, edgecolor=\"black\", label=\"Spoofed\")\n",
    "        matplotlib.pyplot.hist(D1[x,:], bins=40, density=True, alpha=0.4, edgecolor=\"black\", label=\"Authentic\")\n",
    "        matplotlib.pyplot.legend()\n",
    "        matplotlib.pyplot.savefig(\"%s/histogram_%d.png\" % (histograms_folder, x))\n",
    "        matplotlib.pyplot.close()\n",
    "\n",
    "\n",
    "# ----- Plot heatmaps of training set ---\n",
    "def plot_heatmaps (DTR, LTR):\n",
    "    # Consider all samples\n",
    "    corr = numpy.zeros((features, features))\n",
    "    for x in range(features):\n",
    "        for y in range(features):\n",
    "            corr[x][y] = compute_correlation(DTR[x,:], DTR[y,:])\n",
    "    seaborn.set()\n",
    "    heatmap = seaborn.heatmap(numpy.abs(corr), cmap=\"YlGnBu\", linewidth=0.3, square=True, cbar=False)\n",
    "    figure = heatmap.get_figure()\n",
    "    figure.savefig(\"%s/heatmap_all.png\" % (heatmaps_folder))\n",
    "\n",
    "    # Consider only samples labeled as spoofed fingerprints\n",
    "    corr = numpy.zeros((features, features))\n",
    "    for x in range(features):\n",
    "        for y in range(features):\n",
    "            corr[x][y] = compute_correlation(DTR[x,LTR==0], DTR[y,LTR==0])\n",
    "    seaborn.set()\n",
    "    heatmap = seaborn.heatmap(numpy.abs(corr), cmap=\"coolwarm\", linewidth=0.3, square=True, cbar=False)\n",
    "    figure = heatmap.get_figure()\n",
    "    figure.savefig(\"%s/heatmap_spoofed.png\" % (heatmaps_folder))\n",
    "\n",
    "    # Consider only samples labeled as authentic fingerprints\n",
    "    corr = numpy.zeros((features, features))\n",
    "    for x in range(features):\n",
    "        for y in range(features):\n",
    "            corr[x][y] = compute_correlation(DTR[x,LTR==1], DTR[y,LTR==1])\n",
    "    seaborn.set()\n",
    "    heatmap = seaborn.heatmap(numpy.abs(corr), cmap=\"BuPu\", linewidth=0.3, square=True, cbar=False)\n",
    "    figure = heatmap.get_figure()\n",
    "    figure.savefig(\"%s/heatmap_authentic.png\" % (heatmaps_folder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Principal Component Analysis -----\n",
    "def apply_pca (D, L, m, output_folder=None):\n",
    "    # Identify the largest eigenvalues and eigenvectors over to which we project original data\n",
    "    _,U = numpy.linalg.eigh(compute_covariance(D))\n",
    "    P = U[:, ::-1][:, 0:m]\n",
    "    \n",
    "    if output_folder!=None:\n",
    "        # Project data\n",
    "        DP = numpy.dot(P.T, D)\n",
    "        plot_pca_scatters(DP, L, output_folder)\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "# ----- Linear Discriminant Analysis -----\n",
    "def apply_lda (D, L, m, output_folder=None):\n",
    "    # Compute between- and within-class covariance matrices\n",
    "    D0 = D[:, L==0]\n",
    "    D1 = D[:, L==1]\n",
    "    diff0 = compute_mean(D0) - compute_mean(D)\n",
    "    diff1 = compute_mean(D1) - compute_mean(D)\n",
    "    SB = numpy.outer(diff0,diff0) + numpy.outer(diff1,diff1)\n",
    "    SW = compute_covariance(D0) + compute_covariance(D1)\n",
    "    \n",
    "    # Identify the m largest eigenvalues and eigenvectors over to which we project original data\n",
    "    U,s,_ = numpy.linalg.svd(SW)\n",
    "    P1 = numpy.dot(U, row_to_column(1.0/s**0.5)*U.T)\n",
    "    SBt = numpy.dot(P1, numpy.dot(SB, P1.T))\n",
    "    U,_,_ = numpy.linalg.svd(SBt)\n",
    "    P2 = U[:, 0:m]\n",
    "    W = numpy.dot(P1.T, P2)\n",
    "\n",
    "    if output_folder!=None:\n",
    "        # Project data\n",
    "        DP = numpy.dot(W.T, D)\n",
    "        plot_lda_histograms(DP, L, output_folder)\n",
    "\n",
    "\n",
    "# ----- Plot scatters of data projected after PCA -----\n",
    "def plot_pca_scatters (D, L, output_folder):\n",
    "    D0 = D[:, L==0]\n",
    "    D1 = D[:, L==1]\n",
    "\n",
    "    matplotlib.pyplot.figure()\n",
    "    matplotlib.pyplot.scatter(D0[0,:], D0[1,:], label=\"Spoofed\")\n",
    "    matplotlib.pyplot.scatter(D1[0,:], D1[1,:], label=\"Authentic\")\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.savefig(\"%s/scatter_pca.png\" % (output_folder))\n",
    "    matplotlib.pyplot.close()\n",
    "\n",
    "\n",
    "# ----- Plot histograms of data projected after LDA -----\n",
    "def plot_lda_histograms (D, L, output_folder):\n",
    "    # Split according to label\n",
    "    D0 = D[:, L==0]\n",
    "    D1 = D[:, L==1]\n",
    "\n",
    "    # Plot histograms for each feature\n",
    "    matplotlib.pyplot.figure()\n",
    "    matplotlib.pyplot.hist(D0[0,:], bins=40, density=True, alpha=0.4, edgecolor=\"black\", label=\"Spoofed\")\n",
    "    matplotlib.pyplot.hist(D1[0,:], bins=40, density=True, alpha=0.4, edgecolor=\"black\", label=\"Authentic\")\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.savefig(\"%s/histogram_lda.png\" % (output_folder))\n",
    "    matplotlib.pyplot.close()\n",
    "\n",
    "\n",
    "# ----- Plot scatters of data projected after LDA -----\n",
    "def plot_lda_scatters (D, L, output_folder):\n",
    "    D0 = D[:, L==0]\n",
    "    D1 = D[:, L==1]\n",
    "\n",
    "    matplotlib.pyplot.figure()\n",
    "    matplotlib.pyplot.scatter(D0[0,:], D0[1,:], label=\"Spoofed\")\n",
    "    matplotlib.pyplot.scatter(D1[0,:], D1[1,:], label=\"Authentic\")\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.savefig(\"%s/scatter_lda.png\" % (output_folder))\n",
    "    matplotlib.pyplot.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Compute un-normalized Detection Cost Function -----\n",
    "def compute_unnormalized_DCF (pi_t, Cfn, Cfp, ll_ratios, LTE, threshold=None, is_effective=False):\n",
    "    # Define threshold if not previously defined\n",
    "    if threshold is None:\n",
    "        if is_effective is False:\n",
    "            effective_prior = (pi_t*Cfn) / (pi_t*Cfn + (1-pi_t)*Cfp)\n",
    "        else:\n",
    "            effective_prior = pi_t\n",
    "        threshold = -numpy.log(effective_prior / (1-effective_prior))\n",
    "    \n",
    "    # Build confusion matrix\n",
    "    predicted = (ll_ratios>threshold).astype(int)\n",
    "    confusion_matrix = numpy.zeros((2,2))\n",
    "    for i in range(len(LTE)):\n",
    "        confusion_matrix[predicted[i], LTE[i].astype(int)] += 1\n",
    "    \n",
    "    # Compute DCF\n",
    "    fnr = confusion_matrix[0][1] / (confusion_matrix[0][1]+confusion_matrix[1][1])\n",
    "    fpr = confusion_matrix[1][0] / (confusion_matrix[0][0]+confusion_matrix[1][0])\n",
    "    dcf = pi_t*Cfn*fnr + (1-pi_t)*Cfp*fpr\n",
    "\n",
    "    return dcf\n",
    "\n",
    "\n",
    "# ----- Compute normalized Detection Cost Function -----\n",
    "def compute_normalized_DCF (pi_t, Cfn, Cfp, DCFu):\n",
    "    dummy_costs = numpy.array([pi_t*Cfn, (1-pi_t)*Cfp])\n",
    "    index = numpy.argmin(dummy_costs)\n",
    "    dcf = DCFu / dummy_costs[index]\n",
    "    return dcf\n",
    "\n",
    "\n",
    "# ----- Compute actual Detection Cost Function -----\n",
    "def compute_actual_DCF (pi_t, Cfn, Cfp, ll_ratios, LTE, is_effective):\n",
    "    dcfu = compute_unnormalized_DCF(pi_t, Cfn, Cfp, ll_ratios, LTE, None, is_effective)\n",
    "    dcf = compute_normalized_DCF(pi_t, Cfn, Cfp, dcfu)\n",
    "    return dcf\n",
    "\n",
    "\n",
    "# ----- Compute minimum Detection Cost Function -----\n",
    "def compute_min_DCF (pi_t, Cfn, Cfp, ll_ratios, LTE):\n",
    "    dcf_collection = numpy.zeros(ll_ratios.shape)\n",
    "    sorted = numpy.sort(ll_ratios)\n",
    "\n",
    "    for i in range(len(ll_ratios)):\n",
    "        threshold = sorted[i]\n",
    "        unnormalized = compute_unnormalized_DCF(pi_t, Cfn, Cfp, ll_ratios, LTE, threshold)\n",
    "        dcf_collection[i] = compute_normalized_DCF(pi_t, Cfn, Cfp, unnormalized)\n",
    "    return numpy.min(dcf_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Load dataset -----\n",
    "def load_dataset ():\n",
    "    DTR, LTR = load_training_set()\n",
    "    DTE, LTE = load_test_set()\n",
    "    return (DTR,LTR), (DTE,LTE)\n",
    "\n",
    "\n",
    "# ----- Analysis of features -----\n",
    "def features_analysis (DTR, LTR):\n",
    "    plot_histograms(DTR, LTR)\n",
    "    plot_heatmaps(DTR, LTR)\n",
    "\n",
    "\n",
    "# ----- Dimensionality reduction -----\n",
    "def dimensionality_reduction (D, L):\n",
    "    m = 2\n",
    "    apply_pca(D, L, m, \"output/DimensionalityReduction/PCA\")\n",
    "    apply_lda(D, L, m, \"output/DimensionalityReduction/LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Dataset analysis ----\n",
    "(DTR,LTR), (DTE,LTE) = load_dataset()\n",
    "DTR,LTR = randomize(DTR, LTR, 0)\n",
    "features_analysis(DTR, LTR)\n",
    "dimensionality_reduction(DTR, LTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Naive Bayes without tied covariance -----\n",
    "def naive_bayes (DTR, LTR, DTE, LTE):\n",
    "    # Compute mean and covariance for each class\n",
    "    D0 = DTR[:, LTR==0]\n",
    "    D1 = DTR[:, LTR==1]\n",
    "    mu0, cov0 = compute_mean(D0), compute_covariance(D0)*numpy.identity(DTR.shape[0])\n",
    "    mu1, cov1 = compute_mean(D1), compute_covariance(D1)*numpy.identity(DTR.shape[0])\n",
    "\n",
    "    # Compute likelihoods\n",
    "    S = numpy.empty((2,DTE.shape[1]))\n",
    "    for i in range(DTE.shape[1]):\n",
    "        S[0,i] = numpy.exp(logpdf_GAU_ND(row_to_column(DTE[:,i]), mu0, cov0))\n",
    "        S[1,i] = numpy.exp(logpdf_GAU_ND(row_to_column(DTE[:,i]), mu1, cov1))\n",
    "\n",
    "    # Compute log-likelihood ratio and use it to classify samples\n",
    "    llr = numpy.log(S[1,:] / S[0,:])\n",
    "    predicted_labels = predict_labels(llr, 0)\n",
    "    wrong_predictions = count_mispredictions(predicted_labels, LTE)\n",
    "\n",
    "    return wrong_predictions, llr\n",
    "\n",
    "\n",
    "# ----- Naive Bayes with tied covariance -----\n",
    "def tied_naive_bayes (DTR, LTR, DTE, LTE):\n",
    "    # Compute mean for each class and tied covariance matrix\n",
    "    D0 = DTR[:, LTR==0]\n",
    "    D1 = DTR[:, LTR==1]\n",
    "    mu0, cov0 = compute_mean(D0), compute_covariance(D0)*numpy.identity(DTR.shape[0])\n",
    "    mu1, cov1 = compute_mean(D1), compute_covariance(D1)*numpy.identity(DTR.shape[0])\n",
    "    tied_cov = (cov0*D0.shape[1] + cov1*D1.shape[1]) / DTR.shape[1]\n",
    "\n",
    "    # Compute likelihoods\n",
    "    S = numpy.empty((2,DTE.shape[1]))\n",
    "    for i in range(DTE.shape[1]):\n",
    "        S[0,i] = numpy.exp(logpdf_GAU_ND(row_to_column(DTE[:,i]), mu0, tied_cov))\n",
    "        S[1,i] = numpy.exp(logpdf_GAU_ND(row_to_column(DTE[:,i]), mu1, tied_cov))\n",
    "    \n",
    "    # Compute log-likelihood ratio and use it to classify samples\n",
    "    llr = numpy.log(S[1,:] / S[0,:])\n",
    "    predicted_labels = predict_labels(llr, 0)\n",
    "    wrong_predictions = count_mispredictions(predicted_labels, LTE)\n",
    "\n",
    "    return wrong_predictions, llr\n",
    "\n",
    "\n",
    "# ----- Multivariate Gaussian without tied covariance -----\n",
    "def mvg (DTR, LTR, DTE, LTE):\n",
    "    # Compute mean and covariance for each class\n",
    "    D0 = DTR[:, LTR==0]\n",
    "    D1 = DTR[:, LTR==1]\n",
    "    mu0, cov0 = compute_mean(D0), compute_covariance(D0)\n",
    "    mu1, cov1 = compute_mean(D1), compute_covariance(D1)\n",
    "\n",
    "    # Compute likelihoods\n",
    "    S = numpy.empty((2,DTE.shape[1]))\n",
    "    for i in range(DTE.shape[1]):\n",
    "        S[0,i] = numpy.exp(logpdf_GAU_ND(row_to_column(DTE[:,i]), mu0, cov0))\n",
    "        S[1,i] = numpy.exp(logpdf_GAU_ND(row_to_column(DTE[:,i]), mu1, cov1))\n",
    "    \n",
    "    # Compute log-likelihood ratio and use it to classify samples\n",
    "    llr = numpy.log(S[1,:] / S[0,:])\n",
    "    predicted_labels = predict_labels(llr, 0)\n",
    "    wrong_predictions = count_mispredictions(predicted_labels, LTE)\n",
    "\n",
    "    return wrong_predictions, llr\n",
    "\n",
    "\n",
    "# ----- Multivariate Gaussian with tied covariance -----\n",
    "def tied_mvg (DTR, LTR, DTE, LTE):\n",
    "    # Compute mean for each class and tied covariance matrix\n",
    "    D0 = DTR[:, LTR==0]\n",
    "    D1 = DTR[:, LTR==1]\n",
    "    mu0, cov0 = compute_mean(D0), compute_covariance(D0)\n",
    "    mu1, cov1 = compute_mean(D1), compute_covariance(D1)\n",
    "    tied_cov = (cov0*D0.shape[1] + cov1*D1.shape[1]) / DTR.shape[1]\n",
    "\n",
    "    # Compute likelihoods\n",
    "    S = numpy.empty((2,DTE.shape[1]))\n",
    "    for i in range(DTE.shape[1]):\n",
    "        S[0,i] = numpy.exp(logpdf_GAU_ND(row_to_column(DTE[:,i]), mu0, tied_cov))\n",
    "        S[1,i] = numpy.exp(logpdf_GAU_ND(row_to_column(DTE[:,i]), mu1, tied_cov))\n",
    "    \n",
    "    # Compute log-likelihood ratio and use it to classify samples\n",
    "    llr = numpy.log(S[1,:] / S[0,:])\n",
    "    predicted_labels = predict_labels(llr, 0)\n",
    "    wrong_predictions = count_mispredictions(predicted_labels, LTE)\n",
    "\n",
    "    return wrong_predictions, llr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Train model using K-Fold -----\n",
    "def gen_kfold (D, L, K, pca_value, pi_value):\n",
    "    classifiers = [\n",
    "        (mvg, \"MVG\"),\n",
    "        (tied_mvg, \"MVG with tied covariance\"),\n",
    "        (naive_bayes, \"Naive Bayes\"),\n",
    "        (tied_naive_bayes, \"Naive Bayes with tied covariance\")\n",
    "    ]\n",
    "    output_csv = open(output_csv_name, \"a\")\n",
    "    N = int(D.shape[1]/K)\n",
    "\n",
    "    if pca_value==0:\n",
    "        print(\"No PCA, pi: %.3f\\n\" % (pi_value))\n",
    "    else:\n",
    "        print(\"PCA: %d, pi: %.3f\\n\" % (pca_value, pi_value))\n",
    "    \n",
    "    for j,(fun,name) in enumerate(classifiers):\n",
    "        wrong_predictions = 0\n",
    "        numpy.random.seed(j)\n",
    "        ll_ratios = []\n",
    "        indexes = numpy.random.permutation(D.shape[1])\n",
    "\n",
    "        for i in range(K):\n",
    "            # Select which subset to use for evaluation\n",
    "            idxTest = indexes[i*N:(i+1)*N]\n",
    "            if i>0: idxTrainLeft = indexes[0:i*N]\n",
    "            elif (i+1)<K: idxTrainRight = indexes[(i+1)*N:]\n",
    "            if i==0: idxTrain = idxTrainRight\n",
    "            elif (i+1)==K: idxTrain = idxTrainLeft\n",
    "            else: idxTrain = numpy.hstack([idxTrainLeft, idxTrainRight])\n",
    "            DTR,LTR = D[:,idxTrain], L[idxTrain]\n",
    "            DTE,LTE = D[:,idxTest], L[idxTest]\n",
    "\n",
    "            # Apply PCA if necessary\n",
    "            if pca_value!=0:\n",
    "                P = apply_pca(DTR, LTR, pca_value)\n",
    "                DTR,DTE = numpy.dot(P.T, DTR), numpy.dot(P.T, DTE)\n",
    "\n",
    "            # Apply classifier\n",
    "            wrong, llr = fun(DTR, LTR, DTE, LTE)\n",
    "            wrong_predictions += wrong\n",
    "            ll_ratios.append(llr)\n",
    "\n",
    "        # Evaluate accuracy, error rate, and minDCF\n",
    "        error_rate = wrong_predictions / D.shape[1]\n",
    "        accuracy = 1 - error_rate\n",
    "        cost = compute_min_DCF(pi_value, Cfn, Cfp, numpy.hstack(ll_ratios), L)\n",
    "\n",
    "        # Save results in CSV format\n",
    "        output_csv.write(\"%s,%d,_,%.3f,_,_,_,_,_,%.3f,%.3f,%.5f\\n\" % (name, pca_value, pi_value, 100.0*accuracy, 100.0*error_rate, cost))\n",
    "        print(\"%s,%d,_,%.3f,_,_,_,_,_,%.3f,%.3f,%.5f\\n\" % (name, pca_value, pi_value, 100.0*accuracy, 100.0*error_rate, cost))\n",
    "\n",
    "        # Print results to console\n",
    "        print(\"  %s\" % (name))\n",
    "        print(\"    Accuracy: %.3f%%\" % (100.0*accuracy))\n",
    "        print(\"    Error rate: %.3f%%\" % (100.0*error_rate))\n",
    "        print(\"    min DCF: %.3f\" % (cost))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    output_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Logistic Regression objective -----\n",
    "def lr_obj_wrap (DTR, LTR, lambda_value, pi_value):\n",
    "    def lr_obj (v):\n",
    "        w,b = v[0:-1], v[-1]\n",
    "        N = DTR.shape[1]\n",
    "        term1 = lambda_value/2 * numpy.linalg.norm(w)**2\n",
    "        term2 = 0\n",
    "        term3 = 0\n",
    "\n",
    "        nt = DTR[:,LTR==1].shape[1]\n",
    "        nf = N-nt\n",
    "\n",
    "        for i in range(N):\n",
    "            ci = LTR[i]\n",
    "            zi = 2*ci-1\n",
    "            xi = DTR[:,i]\n",
    "            internal_sum = b + numpy.dot(w.T, xi)\n",
    "            internal_prod = -numpy.dot(zi, internal_sum)\n",
    "            if LTR[i]==0: term3 += numpy.logaddexp(0, internal_prod)\n",
    "            else: term2 += numpy.logaddexp(0, internal_prod)\n",
    "        loss = term1 + (pi_value/nt)*term2 + ((1-pi_value)/nf)*term3\n",
    "        return loss\n",
    "    return lr_obj\n",
    "\n",
    "\n",
    "# ----- Compute Logistic Regression scores -----\n",
    "def lr_compute_scores (DTE, v):\n",
    "    s = numpy.empty((DTE.shape[1]))\n",
    "    w,b = v[0:-1], v[-1]\n",
    "    for i in range(DTE.shape[1]):\n",
    "        xt = DTE[:,i]\n",
    "        s[i] = b + numpy.dot(w.T, xt)\n",
    "    return s\n",
    "\n",
    "\n",
    "# ----- Numerical optimization -----\n",
    "def numerical_optimization (function, x0, grad=None):\n",
    "    if grad is None: x,_,_=scipy.optimize.fmin_l_bfgs_b(function,x0,fprime=numpy.gradient(function))\n",
    "    else: x,_,_=scipy.optimize.fmin_l_bfgs_b(function,x0,fprime=grad)\n",
    "    return x\n",
    "\n",
    "\n",
    "# ----- Logistic Regression gradient -----\n",
    "def lr_compute_gradient (DTR, LTR, lambda_value, pi_value):\n",
    "    z = numpy.empty((LTR.shape[0]))\n",
    "    z = 2*LTR-1\n",
    "\n",
    "    def gradient (v):\n",
    "        w,b = v[0:-1], v[-1]\n",
    "        term1 = lambda_value * w\n",
    "        term2 = 0\n",
    "        term3 = 0\n",
    "        \n",
    "        nt = DTR[:, LTR == 1].shape[1]\n",
    "        nf = DTR.shape[1]-nt\n",
    "        \n",
    "        for i in range(DTR.shape[1]):\n",
    "            S = numpy.dot(w.T, DTR[:,i]) + b\n",
    "            zi_si = numpy.dot(z[i], S)\n",
    "            if LTR[i]==1: term2 += numpy.dot(numpy.exp(-zi_si),(numpy.dot(-z[i],DTR[:,i])))/(1+numpy.exp(-zi_si))\n",
    "            else: term3 += numpy.dot(numpy.exp(-zi_si),(numpy.dot(-z[i],DTR[:,i])))/(1+numpy.exp(-zi_si))\n",
    "        dw = term1 + (pi_value/nt)*term2 + (1-pi_value)/(nf)*term3\n",
    "\n",
    "        term1 = 0           \n",
    "        term2 = 0\n",
    "        for i in range(DTR.shape[1]):\n",
    "            S=numpy.dot(w.T,DTR[:,i])+b\n",
    "            zi_si = numpy.dot(z[i], S)\n",
    "            if LTR[i] == 1: term1 += (numpy.exp(-zi_si) * (-z[i]))/(1+numpy.exp(-zi_si))\n",
    "            else: term2 += (numpy.exp(-zi_si) * (-z[i]))/(1+numpy.exp(-zi_si))\n",
    "        db = (pi_value/nt)*term1 + (1-pi_value)/(nf)*term2\n",
    "\n",
    "        return numpy.hstack((dw, db))\n",
    "    return gradient\n",
    "\n",
    "\n",
    "# ----- Linear Logistic Regression -----\n",
    "def linear_logistic_regression (DTR, LTR, DTE, LTE, lam, pi):\n",
    "    x0 = numpy.zeros(DTR.shape[0]+1)\n",
    "    x = numerical_optimization(lr_obj_wrap(DTR, LTR, lam, pi), x0, lr_compute_gradient(DTR, LTR, lam, pi))\n",
    "    \n",
    "    # Compute scores\n",
    "    scores = lr_compute_scores(DTE, x)\n",
    "    predicted_labels = predict_labels(scores, 0)\n",
    "    wrong_predictions = count_mispredictions(predicted_labels, LTE)\n",
    "\n",
    "    return wrong_predictions, scores\n",
    "\n",
    "\n",
    "# ----- Quadratic Logistic Regression -----\n",
    "def quadratic_logistic_regression (DTR, LTR, DTE, LTE, lam, pi):\n",
    "    DTRe = numpy.apply_along_axis(square_and_transpose, 0, DTR)\n",
    "    DTEe = numpy.apply_along_axis(square_and_transpose, 0, DTE)\n",
    "    phi_T = numpy.array(numpy.vstack([DTRe, DTR]))\n",
    "    phi_E = numpy.array(numpy.vstack([DTEe, DTE]))\n",
    "        \n",
    "    x0 = numpy.zeros(phi_T.shape[0]+1)\n",
    "    x = numerical_optimization(lr_obj_wrap(phi_T, LTR, lam, pi), x0, lr_compute_gradient(phi_T, LTR, lam, pi))\n",
    "\n",
    "    # Compute scores and wrong predictions\n",
    "    scores = lr_compute_scores(phi_E, x)\n",
    "    predicted_labels = predict_labels(scores, 0)\n",
    "    wrong_predictions = count_mispredictions(predicted_labels, LTE)\n",
    "\n",
    "    return wrong_predictions, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Train model using K-Fold -----\n",
    "def dis_kfold (D, L, K, pca_value, z_value, pi_value, lambda_value):\n",
    "    classifiers = [\n",
    "        (linear_logistic_regression, \"Linear Logistic Regression\"),\n",
    "        (quadratic_logistic_regression, \"Quadratic Logistic Regression\")\n",
    "    ]\n",
    "    output_csv = open(output_csv_name, \"a\")\n",
    "    N = int(D.shape[1]/K)\n",
    "\n",
    "    if pca_value==0:\n",
    "        if z_value==0:\n",
    "            print(\"No PCA, No ZNorm, pi: %.3f, lambda: %.7f\\n\" % (pi_value, lambda_value))\n",
    "        else:\n",
    "            print(\"No PCA, ZNorm, pi: %.3f, lambda: %.7f\\n\" % (pi_value, lambda_value))\n",
    "    else:\n",
    "        if z_value==0:\n",
    "            print(\"PCA: %d, No ZNorm, pi: %.3f, lambda: %.7f\\n\" % (pca_value, pi_value, lambda_value))\n",
    "        else:\n",
    "            print(\"PCA: %d, ZNorm, pi: %.3f, lambda: %.7f\\n\" % (pca_value, pi_value, lambda_value))\n",
    "    \n",
    "    for j,(fun,name) in enumerate(classifiers):\n",
    "        wrong_predictions = 0\n",
    "        numpy.random.seed(j)\n",
    "        ll_ratios = []\n",
    "        indexes = numpy.random.permutation(D.shape[1])\n",
    "\n",
    "        for i in range(K):\n",
    "            # Select which subset to use for evaluation\n",
    "            idxTest = indexes[i*N:(i+1)*N]\n",
    "            if i>0: idxTrainLeft = indexes[0:i*N]\n",
    "            elif (i+1)<K: idxTrainRight = indexes[(i+1)*N:]\n",
    "            if i==0: idxTrain = idxTrainRight\n",
    "            elif (i+1)==K: idxTrain = idxTrainLeft\n",
    "            else: idxTrain = numpy.hstack([idxTrainLeft, idxTrainRight])\n",
    "            DTR,LTR = D[:,idxTrain], L[idxTrain]\n",
    "            DTE,LTE = D[:,idxTest], L[idxTest]\n",
    "\n",
    "            # Apply ZNorm if necessary\n",
    "            if z_value!=0:\n",
    "                DTR,DTE = compute_znorm(DTR, DTE)\n",
    "\n",
    "            # Apply PCA if necessary\n",
    "            if pca_value!=0:\n",
    "                P = apply_pca(DTR, LTR, pca_value)\n",
    "                DTR,DTE = numpy.dot(P.T, DTR), numpy.dot(P.T, DTE)\n",
    "\n",
    "            # Apply classifier\n",
    "            wrong, scores = fun(DTR, LTR, DTE, LTE, lambda_value, pi_value)\n",
    "            wrong_predictions += wrong\n",
    "            ll_ratios.append(scores)\n",
    "\n",
    "        # Evaluate accuracy, error rate, and minDCF\n",
    "        error_rate = wrong_predictions / D.shape[1]\n",
    "        accuracy = 1 - error_rate\n",
    "        cost = compute_min_DCF(pi_value, Cfn, Cfp, numpy.hstack(ll_ratios), L)\n",
    "\n",
    "        # Save results in CSV format\n",
    "        output_csv.write(\"%s,%d,%d,%.3f,%.7f,_,_,_,_,%.3f,%.3f,%.5f\\n\" % (name, pca_value, z_value, pi_value, lambda_value, 100.0*accuracy, 100.0*error_rate, cost))\n",
    "        print(\"%s,%d,%d,%.3f,%.7f,_,_,_,_,%.3f,%.3f,%.5f\\n\" % (name, pca_value, z_value, pi_value, lambda_value, 100.0*accuracy, 100.0*error_rate, cost))\n",
    "\n",
    "        # Print results to console\n",
    "        print(\"  %s\" % (name))\n",
    "        print(\"    Accuracy: %.3f%%\" % (100.0*accuracy))\n",
    "        print(\"    Error rate: %.3f%%\" % (100.0*error_rate))\n",
    "        print(\"    min DCF: %.3f\" % (cost))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    output_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Train a specific model -----\n",
    "def train_qlr (D, L, K, pca_value, z_value, pi_value, lambda_value):\n",
    "    N = int(D.shape[1]/K)\n",
    "\n",
    "    wrong_predictions = 0\n",
    "    numpy.random.seed(1)\n",
    "    ll_ratios = []\n",
    "    labels = []\n",
    "    indexes = numpy.random.permutation(D.shape[1])\n",
    "\n",
    "    for i in range(K):\n",
    "        # Select which subset to use for evaluation\n",
    "        idxTest = indexes[i*N:(i+1)*N]\n",
    "        if i>0: idxTrainLeft = indexes[0:i*N]\n",
    "        elif (i+1)<K: idxTrainRight = indexes[(i+1)*N:]\n",
    "        if i==0: idxTrain = idxTrainRight\n",
    "        elif (i+1)==K: idxTrain = idxTrainLeft\n",
    "        else: idxTrain = numpy.hstack([idxTrainLeft, idxTrainRight])\n",
    "        DTR,LTR = D[:,idxTrain], L[idxTrain]\n",
    "        DTE,LTE = D[:,idxTest], L[idxTest]\n",
    "\n",
    "        # Apply ZNorm if necessary\n",
    "        if z_value!=0:\n",
    "            DTR,DTE = compute_znorm(DTR, DTE)\n",
    "\n",
    "        # Apply PCA if necessary\n",
    "        if pca_value!=0:\n",
    "            P = apply_pca(DTR, LTR, pca_value)\n",
    "            DTR,DTE = numpy.dot(P.T, DTR), numpy.dot(P.T, DTE)\n",
    "\n",
    "        # Apply classifier\n",
    "        wrong, scores = quadratic_logistic_regression(DTR, LTR, DTE, LTE, lambda_value, pi_value)\n",
    "        wrong_predictions += wrong\n",
    "        ll_ratios.append(scores)\n",
    "        labels.append(LTE)\n",
    "    \n",
    "    return numpy.hstack(ll_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- ___ -----\n",
    "def compute_modified_H (D, L, K):\n",
    "    # Distance\n",
    "    row = numpy.zeros(D.shape[1]) + K\n",
    "    d = numpy.vstack([D, row])\n",
    "    \n",
    "    # Assign a class label to each sample\n",
    "    z = numpy.zeros(len(L))\n",
    "    for i in range(len(L)):\n",
    "        if (L[i]==0): z[i] = -1\n",
    "        else: z[i] = 1\n",
    "    \n",
    "    G_ij = numpy.dot(d.T, d)\n",
    "    z_ij = numpy.dot(row_to_column(z), column_to_row(z))\n",
    "    H = z_ij * G_ij\n",
    "\n",
    "    return d,z,H\n",
    "\n",
    "\n",
    "# ----- ___ -----\n",
    "def compute_kernel_H (D, L, K):\n",
    "    # Assign a class label to each sample\n",
    "    z = numpy.zeros(len(L))\n",
    "    for i in range(len(L)):\n",
    "        if (L[i]==0): z[i] = -1\n",
    "        else: z[i] = 1\n",
    "    \n",
    "    ker = (numpy.dot(D.T, D)+1)**2 + K**2\n",
    "    p1 = numpy.dot(row_to_column(z), column_to_row(z))\n",
    "    H = p1 * ker\n",
    "\n",
    "    return z,H\n",
    "\n",
    "\n",
    "# ----- ___ -----\n",
    "def compute_polynomial_kernel_H (D, L, c, d, K):\n",
    "    # Assign a class label to each sample\n",
    "    z = numpy.zeros(len(L))\n",
    "    for i in range(len(L)):\n",
    "        if (L[i]==0): z[i] = -1\n",
    "        else: z[i] = 1\n",
    "    \n",
    "    ker = (numpy.dot(D.T, D)+c)**d + K**2\n",
    "    p1 = numpy.dot(row_to_column(z), column_to_row(z))\n",
    "    H = p1 * ker\n",
    "\n",
    "    return z,H\n",
    "\n",
    "\n",
    "# ----- ___ -----\n",
    "def compute_radial_kernel_H (D, L, gamma, K):\n",
    "    # Assign a class label to each sample\n",
    "    z = numpy.zeros(len(L))\n",
    "    for i in range(len(L)):\n",
    "        if (L[i]==0): z[i] = -1\n",
    "        else: z[i] = 1\n",
    "\n",
    "    ker = numpy.zeros((D.shape[1], D.shape[1]))\n",
    "    for i in range(D.shape[1]):\n",
    "        for j in range(D.shape[1]):\n",
    "            ker[i,j] = numpy.exp(-gamma * (numpy.linalg.norm(D[:,i]-D[:,j])**2)) + K**2\n",
    "    p1 = numpy.dot(row_to_column(z), column_to_row(z))\n",
    "    H = p1 * ker\n",
    "\n",
    "    return z,H\n",
    "\n",
    "\n",
    "# ----- Compute gradient -----\n",
    "def compute_gradient (a, H):\n",
    "    p1 = numpy.dot(H, row_to_column(a))\n",
    "    p2 = numpy.dot(column_to_row(a), p1)\n",
    "    s = a.sum()\n",
    "    return 0.5*p2.ravel()-s, p1.ravel()-numpy.ones(a.size)\n",
    "\n",
    "\n",
    "# ----- Optimal dual solution -----\n",
    "def minimize_dual (D, z, H, C):\n",
    "    bounds = [(0,C)] * D.shape[1]\n",
    "    alpha, dual, _ = scipy.optimize.fmin_l_bfgs_b(compute_gradient, numpy.zeros(D.shape[1]), args=(H,), bounds=bounds, factr=1.0)\n",
    "    return alpha, -dual\n",
    "\n",
    "\n",
    "# ----- Find optimal primal solution -----\n",
    "def primal_model (D, alpha, z, K, DTE, LTE):\n",
    "    w = numpy.dot(D, row_to_column(alpha) * row_to_column(z))\n",
    "    S = numpy.dot(column_to_row(w), D)\n",
    "    loss = numpy.maximum(numpy.zeros(S.shape), 1-z*S).sum()\n",
    "\n",
    "    row = numpy.zeros(DTE.shape[1]) + K\n",
    "    DTEe = numpy.vstack([DTE, row])\n",
    "    Se = numpy.dot(w.T, DTEe)\n",
    "    predicted_labels = 1 * (Se>0)\n",
    "    wrong = numpy.array(predicted_labels!=LTE).sum()\n",
    "\n",
    "    return wrong, Se.ravel()\n",
    "\n",
    "\n",
    "# ----- ___ -----\n",
    "def primal_model_kernel (DTR, alpha, z, K, DTE, LTE):\n",
    "    S = numpy.sum(numpy.dot(column_to_row(alpha*z), (numpy.dot(DTR.T, DTE)+1)**2 + K), axis=0)\n",
    "    predicted_labels = 1 * (S>0)\n",
    "    wrong = numpy.array(predicted_labels!=LTE).sum()\n",
    "\n",
    "    return wrong, S.ravel()\n",
    "\n",
    "\n",
    "# ----- Linear SVM ---\n",
    "def linear_svm (DTR, LTR, DTE, LTE, K, C, gamma):\n",
    "    D,z,H = compute_modified_H(DTR, LTR, K)\n",
    "    alpha, dual = minimize_dual(D, z, H, C)\n",
    "    wrong, scores = primal_model(D, alpha, z, K, DTE, LTE)\n",
    "    return wrong, scores\n",
    "\n",
    "\n",
    "# ----- Quadratic SVM -----\n",
    "def quadratic_svm (DTR, LTR, DTE, LTE, K, C, gamma):\n",
    "    z,H = compute_kernel_H(DTR, LTR, K)\n",
    "    alpha, dual = minimize_dual(DTR, z, H, C)\n",
    "    wrong, scores = primal_model_kernel(DTR, alpha, z, K, DTE, LTE)\n",
    "    return wrong, scores\n",
    "\n",
    "\n",
    "# ----- Polynomial Kernel SVM -----\n",
    "def polynomial_kernel_svm (DTR, LTR, DTE, LTE, K, C, gamma):\n",
    "    c = 1\n",
    "    d = 2\n",
    "    z,H = compute_polynomial_kernel_H(DTR, LTR, c, d, K)\n",
    "    alpha, dual = minimize_dual(DTR, z, H, C)\n",
    "    scores = numpy.sum(numpy.dot(column_to_row(alpha*z), (numpy.dot(DTR.T, DTE)+c)**d+K), axis=0)\n",
    "    predicted_labels = 1*(scores>0)\n",
    "    wrong = numpy.array(predicted_labels!=LTE).sum()\n",
    "    return wrong, scores\n",
    "\n",
    "\n",
    "# ----- Radial Basis Function Kernel SVM -----\n",
    "def radial_kernel_svm (DTR, LTR, DTE, LTE, K, C, gamma):\n",
    "    z,H = compute_radial_kernel_H(DTR, LTR, gamma, K)\n",
    "    alpha, dual = minimize_dual(DTR, z, H, C)\n",
    "    ker = numpy.zeros((DTR.shape[1], DTE.shape[1]))\n",
    "    for i in range(DTR.shape[1]):\n",
    "        for j in range(DTE.shape[1]):\n",
    "            ker[i,j] = numpy.exp(-gamma * (numpy.linalg.norm(DTR[:,i]-DTE[:,j])**2)) + K**2\n",
    "    scores = numpy.sum(numpy.dot((column_to_row(alpha*z)), ker), axis=0)\n",
    "    predicted_labels = 1*(scores>0)\n",
    "    wrong = numpy.array(predicted_labels!=LTE).sum()\n",
    "    return wrong, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Train model using K-Fold -----\n",
    "def svm_kfold (D, L, K, pca_value, z_value, C_value, gamma_value):\n",
    "    classifiers = [\n",
    "        (linear_svm, \"Linear SVM\"),\n",
    "        (quadratic_svm, \"Quadratic SVM\"),\n",
    "        (polynomial_kernel_svm, \"Polynomial Kernel SVM\"),\n",
    "        (radial_kernel_svm, \"Radial Basis Function Kernel SVM\")\n",
    "    ]\n",
    "    output_csv = open(output_csv_name, \"a\")\n",
    "    N = int(D.shape[1]/K)\n",
    "\n",
    "    if pca_value==0:\n",
    "        if z_value==0:\n",
    "            print(\"No PCA, No ZNorm, C: %.5f, gamma: %.5f\\n\" % (C_value, gamma_value))\n",
    "        else:\n",
    "            print(\"No PCA, ZNorm, C: %.5f, gamma: %.5f\\n\" % (C_value, gamma_value))\n",
    "    else:\n",
    "        if z_value==0:\n",
    "            print(\"PCA: %d, C: %.5f, No ZNorm, gamma: %.5f\\n\" % (pca_value, C_value, gamma_value))\n",
    "        else:\n",
    "            print(\"PCA: %d, C: %.5f, ZNorm, gamma: %.5f\\n\" % (pca_value, C_value, gamma_value))\n",
    "\n",
    "    for j,(fun,name) in enumerate(classifiers):\n",
    "        wrong_predictions = 0\n",
    "        numpy.random.seed(j)\n",
    "        ll_ratios = []\n",
    "        indexes = numpy.random.permutation(D.shape[1])\n",
    "\n",
    "        for i in range(K):\n",
    "            # Select which subset to use for evaluation\n",
    "            idxTest = indexes[i*N:(i+1)*N]\n",
    "            if i>0: idxTrainLeft = indexes[0:i*N]\n",
    "            elif (i+1)<K: idxTrainRight = indexes[(i+1)*N:]\n",
    "            if i==0: idxTrain = idxTrainRight\n",
    "            elif (i+1)==K: idxTrain = idxTrainLeft\n",
    "            else: idxTrain = numpy.hstack([idxTrainLeft, idxTrainRight])\n",
    "            DTR,LTR = D[:,idxTrain], L[idxTrain]\n",
    "            DTE,LTE = D[:,idxTest], L[idxTest]\n",
    "\n",
    "            # Apply ZNorm if necessary\n",
    "            if z_value!=0:\n",
    "                DTR,DTE = compute_znorm(DTR, DTE)\n",
    "\n",
    "            # Apply PCA if necessary\n",
    "            if pca_value!=0:\n",
    "                P = apply_pca(DTR, LTR, pca_value)\n",
    "                DTR,DTE = numpy.dot(P.T, DTR), numpy.dot(P.T, DTE)\n",
    "            \n",
    "            # Apply classifier\n",
    "            wrong, scores = fun(DTR, LTR, DTE, LTE, K, C_value, gamma_value)\n",
    "            wrong_predictions += wrong\n",
    "            ll_ratios.append(scores)\n",
    "        \n",
    "        # Evaluate accuracy, error rate, and minDCF\n",
    "        error_rate = wrong_predictions / D.shape[1]\n",
    "        accuracy = 1 - error_rate\n",
    "        cost = compute_min_DCF(pi_t, Cfn, Cfp, numpy.hstack(ll_ratios), L)\n",
    "\n",
    "        # Save results in CSV format\n",
    "        output_csv.write(\"%s,%d,%d,_,_,%.5f,%.5f,_,_,%.3f,%.3f,%.5f\\n\" % (name, pca_value, z_value, C_value, gamma_value, 100.0*accuracy, 100.0*error_rate, cost))\n",
    "        print(\"%s,%d,%d,_,_,%.5f,%.5f,_,_,%.3f,%.3f,%.5f\\n\" % (name, pca_value, z_value, C_value, gamma_value, 100.0*accuracy, 100.0*error_rate, cost))\n",
    "\n",
    "        # Print results to console\n",
    "        print(\"  %s\" % (name))\n",
    "        print(\"    Accuracy: %.3f%%\" % (100.0*accuracy))\n",
    "        print(\"    Error rate: %.3f%%\" % (100.0*error_rate))\n",
    "        print(\"    min DCF: %.3f\" % (cost))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    output_csv.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Train a specific model -----\n",
    "def train_pol_svm (D, L, K, pca_value, z_value, C_value, gamma_value):\n",
    "    N = int(D.shape[1]/K)\n",
    "\n",
    "    wrong_predictions = 0\n",
    "    numpy.random.seed(1)\n",
    "    ll_ratios = []\n",
    "    indexes = numpy.random.permutation(D.shape[1])\n",
    "\n",
    "    for i in range(K):\n",
    "        # Select which subset to use for evaluation\n",
    "        idxTest = indexes[i*N:(i+1)*N]\n",
    "        if i>0: idxTrainLeft = indexes[0:i*N]\n",
    "        elif (i+1)<K: idxTrainRight = indexes[(i+1)*N:]\n",
    "        if i==0: idxTrain = idxTrainRight\n",
    "        elif (i+1)==K: idxTrain = idxTrainLeft\n",
    "        else: idxTrain = numpy.hstack([idxTrainLeft, idxTrainRight])\n",
    "        DTR,LTR = D[:,idxTrain], L[idxTrain]\n",
    "        DTE,LTE = D[:,idxTest], L[idxTest]\n",
    "\n",
    "        # Apply ZNorm if necessary\n",
    "        if z_value!=0:\n",
    "            DTR,DTE = compute_znorm(DTR, DTE)\n",
    "\n",
    "        # Apply PCA if necessary\n",
    "        if pca_value!=0:\n",
    "            P = apply_pca(DTR, LTR, pca_value)\n",
    "            DTR,DTE = numpy.dot(P.T, DTR), numpy.dot(P.T, DTE)\n",
    "            \n",
    "        # Apply classifier\n",
    "        wrong, scores = polynomial_kernel_svm(DTR, LTR, DTE, LTE, K, C_value, gamma_value)\n",
    "        wrong_predictions += wrong\n",
    "        ll_ratios.append(scores)\n",
    "\n",
    "    return numpy.hstack(ll_ratios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- ___ -----\n",
    "def logpdf_GAU_ND_GMM(X, mu, C):\n",
    "    M,N = X.shape[0], X.shape[1]\n",
    "    Y = numpy.empty([1,N])\n",
    "    \n",
    "    for i in range(N):\n",
    "        x = X[:,i:i+1]\n",
    "        _,det = numpy.linalg.slogdet(C)\n",
    "        inv = numpy.linalg.inv(C)\n",
    "        density = -(M/2)*numpy.log(2*math.pi) - 1/2*det - 1/2*numpy.dot((x-mu).T, numpy.dot(inv, (x-mu)))\n",
    "        Y[:,i]=density\n",
    "        \n",
    "    return Y.ravel()\n",
    "\n",
    "\n",
    "# ----- Compute log-density of a GMM for a set of samples contained in matrix X -----\n",
    "def logpdf_GMM (x, gmm):    \n",
    "    y = []\n",
    "    for weight,mu,sigma in gmm:\n",
    "        lc = logpdf_GAU_ND_GMM(x, mu, sigma) + numpy.log(weight)\n",
    "        y.append(column_to_row(lc))\n",
    "    S = numpy.vstack(y)\n",
    "    logdensity = scipy.special.logsumexp(y, axis=0)\n",
    "    return S,logdensity\n",
    "\n",
    "\n",
    "# ----- ___ -----\n",
    "def ML_GMM_iteration(D, gmm, diag, tied):\n",
    "    prevLL = None\n",
    "    oldLL = None\n",
    "    deltaLL = 1.0\n",
    "    iteration = 0\n",
    "\n",
    "    while deltaLL>1e-6:\n",
    "        componentsLL = []\n",
    "        for w, mu, C in gmm:\n",
    "            ll = logpdf_GAU_ND_GMM(D, mu, C) + numpy.log(w)\n",
    "            componentsLL.append(column_to_row(ll))\n",
    "        LL = numpy.vstack(componentsLL)\n",
    "        post = numpy.exp(LL - scipy.special.logsumexp(LL, axis=0))\n",
    "        oldLL = prevLL\n",
    "        prevLL = scipy.special.logsumexp(LL, axis=0).sum() / D.shape[1]\n",
    "        \n",
    "        if oldLL is not None: deltaLL = prevLL - oldLL\n",
    "        iteration += 1\n",
    "        psi = 0.01\n",
    "        updatedGMM = []\n",
    "        for i in range(post.shape[0]):\n",
    "            Z = post[i].sum()\n",
    "            F = (post[i:i+1, :]*D).sum(1)\n",
    "            S = numpy.dot((post[i:i+1, :])*D, D.T)\n",
    "            new_weight = Z / D.shape[1]\n",
    "            new_mean = row_to_column(F/Z)\n",
    "            new_sigma = S/Z - numpy.dot(new_mean, new_mean.T)\n",
    "            \n",
    "            if tied:\n",
    "                c = 0\n",
    "                for j in range(post.shape[0]):\n",
    "                    Z = post[j].sum()\n",
    "                    F = (post[j:j+1, :]*D).sum(1)\n",
    "                    S = numpy.dot((post[j:j+1, :])*D, D.T)\n",
    "                    c += Z * (S/Z - numpy.dot(row_to_column(F/Z), row_to_column(F/Z).T))\n",
    "                new_sigma = c / D.shape[1]\n",
    "            \n",
    "            if diag: new_sigma = new_sigma * numpy.eye(new_sigma.shape[0])\n",
    "            \n",
    "            U, s, _ = numpy.linalg.svd(new_sigma)\n",
    "            s[s<psi] = psi\n",
    "            new_sigma=numpy.dot(U, row_to_column(s)*U.T)\n",
    "            updatedGMM.append((new_weight, new_mean, new_sigma))\n",
    "\n",
    "        gmm = updatedGMM\n",
    "        componentsLL = []\n",
    "        for w, mu, C in gmm:\n",
    "            ll = logpdf_GAU_ND_GMM(D, mu, C) + numpy.log(w)\n",
    "            componentsLL.append(column_to_row(ll))\n",
    "        LL = numpy.vstack(componentsLL)\n",
    "        post = LL - scipy.special.logsumexp(LL, axis=0)\n",
    "        post = numpy.exp(post)\n",
    "        oldLL = prevLL\n",
    "        prevLL = scipy.special.logsumexp(LL, axis=0).sum() / D.shape[1]\n",
    "        deltaLL = prevLL - oldLL\n",
    "    \n",
    "    return gmm\n",
    "\n",
    "\n",
    "# ----- ___ -----\n",
    "def ML_GMM_LBG (D, weights, means, sigma, G, diag, tied):\n",
    "    gmm = [(weights,means,sigma)]\n",
    "\n",
    "    while len(gmm)<=G:\n",
    "        if len(gmm)!=1: gmm = ML_GMM_iteration(D, gmm, diag, tied)\n",
    "        if len(gmm)==G: break\n",
    "\n",
    "        newGMM = []\n",
    "        for(weight, mu, sigma) in gmm:\n",
    "            U,s,_ = numpy.linalg.svd(sigma)\n",
    "            s[s<0.01] = 0.01\n",
    "            sigma = numpy.dot(U, row_to_column(s)*U.T)\n",
    "            \n",
    "            newGMM.append((weight*0.5, mu+s[0]**0.5*U[:, 0:1]*0.1, sigma)) \n",
    "            newGMM.append((weight*0.5, mu-s[0]**0.5*U[:, 0:1]*0.1, sigma))\n",
    "        gmm = newGMM\n",
    "\n",
    "    return gmm\n",
    "\n",
    "\n",
    "# ----- GMM classifier - Full covariance -----\n",
    "def gmm_full_covariance (DTR, LTR, DTE, LTE, g0, g1):\n",
    "    # Consider only labels=0\n",
    "    D0 = DTR[:,LTR==0]\n",
    "    w0 = 1.0\n",
    "    mu0 = compute_mean(D0)\n",
    "    sigma0 = compute_covariance(D0)\n",
    "    U,s,_ = numpy.linalg.svd(sigma0)\n",
    "    s[s<0.01] = 0.01\n",
    "    C0 = numpy.dot(U, row_to_column(s)*U.T)\n",
    "    gmm0 = ML_GMM_LBG(D0, w0, mu0, C0, g0, False, False)\n",
    "    _,score0 = logpdf_GMM(DTE, gmm0)\n",
    "\n",
    "    # Consider only labels=1\n",
    "    D1 = DTR[:,LTR==1]\n",
    "    w1 = 1.0\n",
    "    mu1 = compute_mean(D1)\n",
    "    sigma1 = compute_covariance(D1)\n",
    "    U,s,_ = numpy.linalg.svd(sigma1)\n",
    "    s[s<0.01] = 0.01\n",
    "    C1 = numpy.dot(U, row_to_column(s)*U.T)\n",
    "    gmm1 = ML_GMM_LBG(D1, w1, mu1, C1, g1, False, False)\n",
    "    _,score1 = logpdf_GMM(DTE, gmm1)\n",
    "\n",
    "    # Compute scores and wrong predictions\n",
    "    scores = numpy.vstack((score0, score1))\n",
    "    marginals = column_to_row(scipy.special.logsumexp(scores, axis=0))\n",
    "    f = numpy.exp(scores - marginals)\n",
    "    wrong_predictions = (f.argmax(0)!=LTE).sum()\n",
    "    scores = (score1-score0)[0]\n",
    "\n",
    "    return wrong_predictions, scores\n",
    "\n",
    "\n",
    "# ----- GMM classifier - Diagonal covariance -----\n",
    "def gmm_diagonal_covariance (DTR, LTR, DTE, LTE, g0, g1):\n",
    "    # Consider only labels=0\n",
    "    D0 = DTR[:,LTR==0]\n",
    "    w0 = 1.0\n",
    "    mu0 = compute_mean(D0)\n",
    "    sigma0 = compute_covariance(D0)\n",
    "    sigma0 *= numpy.eye(sigma0.shape[0])\n",
    "    U,s,_ = numpy.linalg.svd(sigma0)\n",
    "    s[s<0.01] = 0.01\n",
    "    C0 = numpy.dot(U, row_to_column(s)*U.T)\n",
    "    gmm0 = ML_GMM_LBG(D0, w0, mu0, C0, g0, True, False)\n",
    "    _,score0 = logpdf_GMM(DTE, gmm0)\n",
    "\n",
    "    # Consider only labels=1\n",
    "    D1 = DTR[:,LTR==1]\n",
    "    w1 = 1.0\n",
    "    mu1 = compute_mean(D1)\n",
    "    sigma1 = compute_covariance(D1)\n",
    "    sigma1 *= numpy.eye(sigma1.shape[0])\n",
    "    U,s,_ = numpy.linalg.svd(sigma1)\n",
    "    s[s<0.01] = 0.01\n",
    "    C1 = numpy.dot(U, row_to_column(s)*U.T)\n",
    "    gmm1 = ML_GMM_LBG(D1, w1, mu1, C1, g1, True, False)\n",
    "    _,score1 = logpdf_GMM(DTE, gmm1)\n",
    "\n",
    "    # Compute scores and wrong predictions\n",
    "    scores = numpy.vstack((score0, score1))\n",
    "    marginals = column_to_row(scipy.special.logsumexp(scores, axis=0))\n",
    "    f = numpy.exp(scores - marginals)\n",
    "    wrong_predictions = (f.argmax(0)!=LTE).sum()\n",
    "    scores = (score1-score0)[0]\n",
    "\n",
    "    return wrong_predictions, scores\n",
    "\n",
    "\n",
    "# ----- GMM classifier - Tied covariance -----\n",
    "def gmm_tied_covariance (DTR, LTR, DTE, LTE, g0, g1):\n",
    "    # Consider only labels=0\n",
    "    D0 = DTR[:,LTR==0]\n",
    "    w0 = 1.0\n",
    "    mu0 = compute_mean(D0)\n",
    "    sigma0 = compute_covariance(D0)\n",
    "    U,s,_ = numpy.linalg.svd(sigma0)\n",
    "    s[s<0.01] = 0.01\n",
    "    C0 = numpy.dot(U, row_to_column(s)*U.T)\n",
    "    gmm0 = ML_GMM_LBG(D0, w0, mu0, C0, g0, False, True)\n",
    "    _,score0 = logpdf_GMM(DTE, gmm0)\n",
    "\n",
    "    # Consider only labels=1\n",
    "    D1 = DTR[:,LTR==1]\n",
    "    w1 = 1.0\n",
    "    mu1 = compute_mean(D1)\n",
    "    sigma1 = compute_covariance(D1)\n",
    "    U,s,_ = numpy.linalg.svd(sigma1)\n",
    "    s[s<0.01] = 0.01\n",
    "    C1 = numpy.dot(U, row_to_column(s)*U.T)\n",
    "    gmm1 = ML_GMM_LBG(D1, w1, mu1, C1, g1, False, True)\n",
    "    _,score1 = logpdf_GMM(DTE, gmm1)\n",
    "\n",
    "    # Compute scores and wrong predictions\n",
    "    scores = numpy.vstack((score0, score1))\n",
    "    marginals = column_to_row(scipy.special.logsumexp(scores, axis=0))\n",
    "    f = numpy.exp(scores - marginals)\n",
    "    wrong_predictions = (f.argmax(0)!=LTE).sum()\n",
    "    scores = (score1-score0)[0]\n",
    "\n",
    "    return wrong_predictions, scores\n",
    "\n",
    "\n",
    "# ----- GMM classifier - Tied diagonal covariance -----\n",
    "def gmm_tied_diagonal_covariance (DTR, LTR, DTE, LTE, g0, g1):\n",
    "    # Consider only labels=0\n",
    "    D0 = DTR[:,LTR==0]\n",
    "    w0 = 1.0\n",
    "    mu0 = compute_mean(D0)\n",
    "    sigma0 = compute_covariance(D0)\n",
    "    sigma0 *= numpy.eye(sigma0.shape[0])\n",
    "    U,s,_ = numpy.linalg.svd(sigma0)\n",
    "    s[s<0.01] = 0.01\n",
    "    C0 = numpy.dot(U, row_to_column(s)*U.T)\n",
    "    gmm0 = ML_GMM_LBG(D0, w0, mu0, C0, g0, True, True)\n",
    "    _,score0 = logpdf_GMM(DTE, gmm0)\n",
    "\n",
    "    # Consider only labels=1\n",
    "    D1 = DTR[:,LTR==1]\n",
    "    w1 = 1.0\n",
    "    mu1 = compute_mean(D1)\n",
    "    sigma1 = compute_covariance(D1)\n",
    "    sigma1 *= numpy.eye(sigma1.shape[0])\n",
    "    U,s,_ = numpy.linalg.svd(sigma1)\n",
    "    s[s<0.01] = 0.01\n",
    "    C1 = numpy.dot(U, row_to_column(s)*U.T)\n",
    "    gmm1 = ML_GMM_LBG(D1, w1, mu1, C1, g1, True, True)\n",
    "    _,score1 = logpdf_GMM(DTE, gmm1)\n",
    "\n",
    "    # Compute scores and wrong predictions\n",
    "    joint = numpy.vstack((score0, score1))\n",
    "    marginals = column_to_row(scipy.special.logsumexp(joint, axis=0))\n",
    "    f = numpy.exp(joint - marginals)\n",
    "    wrong_predictions = (f.argmax(0)!=LTE).sum()\n",
    "    scores = (score1-score0)[0]\n",
    "\n",
    "    return wrong_predictions, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- GMM training -----\n",
    "def gmm_kfold (D, L, K, pca_value, z_value, g0_value, g1_value):\n",
    "    classifiers = [\n",
    "        (gmm_full_covariance, \"GMM Full Covariance\"),\n",
    "        (gmm_diagonal_covariance, \"GMM Diagonal Covariance\"),\n",
    "        (gmm_tied_covariance, \"GMM Tied Covariance\"),\n",
    "        (gmm_tied_diagonal_covariance, \"GMM Tied Diagonal Covariance\")\n",
    "    ]\n",
    "    output_csv = open(output_csv_name, \"a\")\n",
    "    N = int(D.shape[1]/K)\n",
    "\n",
    "    if pca_value==0:\n",
    "        if z_value==0:\n",
    "            print(\"No PCA, No ZNorm, G0: %d, G1: %d\\n\" % (g0_value, g1_value))\n",
    "        else:\n",
    "            print(\"No PCA, ZNorm, G0: %d, G1: %d\\n\" % (g0_value, g1_value))\n",
    "    else:\n",
    "        if z_value==0:\n",
    "            print(\"PCA: %d, No ZNorm, G0: %d, G1: %d\\n\" % (pca_value, g0_value, g1_value))\n",
    "        else:\n",
    "            print(\"PCA: %d, ZNorm, G0: %d, G1: %d\\n\" % (pca_value, g0_value, g1_value))\n",
    "\n",
    "    for j,(fun,name) in enumerate(classifiers):\n",
    "        wrong_predictions = 0\n",
    "        numpy.random.seed(j)\n",
    "        scores = []\n",
    "        labels = []\n",
    "        indexes = numpy.random.permutation(D.shape[1])\n",
    "\n",
    "        for i in range(K):\n",
    "            # Select which subset to use for evaluation\n",
    "            idxTest = indexes[i*N:(i+1)*N]\n",
    "            if i>0: idxTrainLeft = indexes[0:i*N]\n",
    "            elif (i+1)<K: idxTrainRight = indexes[(i+1)*N:]\n",
    "            if i==0: idxTrain = idxTrainRight\n",
    "            elif (i+1)==K: idxTrain = idxTrainLeft\n",
    "            else: idxTrain = numpy.hstack([idxTrainLeft, idxTrainRight])\n",
    "            DTR,LTR = D[:,idxTrain], L[idxTrain]\n",
    "            DTE,LTE = D[:,idxTest], L[idxTest]\n",
    "\n",
    "            # Apply Z-Normalization if necessary\n",
    "            if z_value!=0:\n",
    "                DTR,DTE = compute_znorm(DTR, DTE)\n",
    "\n",
    "            # Apply PCA if necessary\n",
    "            if pca_value!=0:\n",
    "                P = apply_pca(DTR, LTR, pca_value)\n",
    "                DTR,DTE = numpy.dot(P.T, DTR), numpy.dot(P.T, DTE)\n",
    "            \n",
    "            # Apply classifier\n",
    "            wrong, score = fun(DTR, LTR, DTE, LTE, g0_value, g1_value)\n",
    "            wrong_predictions += wrong\n",
    "            scores.append(score)\n",
    "            labels.append(LTE)\n",
    "        \n",
    "        # Evaluate accuracy and error rate\n",
    "        error_rate = wrong_predictions / D.shape[1]\n",
    "        accuracy = 1 - error_rate\n",
    "        cost = compute_min_DCF(pi_t, Cfn, Cfp, numpy.hstack(scores), numpy.hstack(labels))\n",
    "\n",
    "        # Save results in CSV format\n",
    "        output_csv.write(\"%s,%d,%d,_,_,_,_,%d,%d,%.3f,%.3f,%.5f\\n\" % (name, pca_value, z_value, g0_value, g1_value, 100.0*accuracy, 100.0*error_rate, cost))\n",
    "        print(\"%s,%d,%d,_,_,_,_,%d,%d,%.3f,%.3f,%.5f\\n\" % (name, pca_value, z_value, g0_value, g1_value, 100.0*accuracy, 100.0*error_rate, cost))\n",
    "\n",
    "        # Print results to console\n",
    "        print(\"  %s\" % (name))\n",
    "        print(\"    Accuracy: %.3f%%\" % (100.0*accuracy))\n",
    "        print(\"    Error rate: %.3f%%\" % (100.0*error_rate))\n",
    "        print(\"    min DCF: %.3f\\n\" % (cost))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    output_csv.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Train a specific model -----\n",
    "def train_diagonal_gmm (D, L, K, pca_value, z_value, g0_value, g1_value):\n",
    "    N = int(D.shape[1]/K)\n",
    "\n",
    "    wrong_predictions = 0\n",
    "    numpy.random.seed(1)\n",
    "    scores = []\n",
    "    labels = []\n",
    "    indexes = numpy.random.permutation(D.shape[1])\n",
    "\n",
    "    for i in range(K):\n",
    "        # Select which subset to use for evaluation\n",
    "        idxTest = indexes[i*N:(i+1)*N]\n",
    "        if i>0: idxTrainLeft = indexes[0:i*N]\n",
    "        elif (i+1)<K: idxTrainRight = indexes[(i+1)*N:]\n",
    "        if i==0: idxTrain = idxTrainRight\n",
    "        elif (i+1)==K: idxTrain = idxTrainLeft\n",
    "        else: idxTrain = numpy.hstack([idxTrainLeft, idxTrainRight])\n",
    "        DTR,LTR = D[:,idxTrain], L[idxTrain]\n",
    "        DTE,LTE = D[:,idxTest], L[idxTest]\n",
    "\n",
    "        # Apply Z-Normalization if necessary\n",
    "        if z_value!=0:\n",
    "            DTR,DTE = compute_znorm(DTR, DTE)\n",
    "\n",
    "        # Apply PCA if necessary\n",
    "        if pca_value!=0:\n",
    "            P = apply_pca(DTR, LTR, pca_value)\n",
    "            DTR,DTE = numpy.dot(P.T, DTR), numpy.dot(P.T, DTE)\n",
    "            \n",
    "        # Apply classifier\n",
    "        wrong, score = gmm_diagonal_covariance(DTR, LTR, DTE, LTE, g0_value, g1_value)\n",
    "        wrong_predictions += wrong\n",
    "        scores.append(score)\n",
    "        labels.append(LTE)\n",
    "\n",
    "    return numpy.hstack(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Train model -----\n",
    "def train_model (D, L):\n",
    "    K = 5\n",
    "\n",
    "    output_csv = open(output_csv_name, \"w\")\n",
    "    output_csv.write(\"Model,PCA,ZNorm,pi,lambda,C,gamma,G0,G1,Accuracy,Error rate,minDCF\\n\")\n",
    "    output_csv.close()\n",
    "    \n",
    "    pca_values = [0, 9, 8, 7, 6, 5]         # value=0 when we don't apply PCA\n",
    "    pi_values = [0.1, 0.5, 0.9, effective_prior]\n",
    "    lambda_values = [1e-6, 1e-4, 1e-3, 1e-1, 1e+0, 1e+1, 1e+2]\n",
    "    gmm_values = [2, 4, 8]\n",
    "    C_values = [1e-4, 1e-3, 1e-2, 1e-1, 1e+0]\n",
    "    gamma_values = [1e-4, 1e-3, 1e-2, 1e-1, 1e+0]\n",
    "    z_values = [0, 1]\n",
    "    \n",
    "    # pca_values = [0, 9]                     # value=0 when we don't apply PCA\n",
    "    # pi_values = [0.1, 0.5]\n",
    "    # lambda_values = [1e-6, 1e-1]\n",
    "    # gmm_values = [2, 8]\n",
    "    # C_values = [1e-2, 1e+0]\n",
    "    # gamma_values = [1e-3, 1e-1]\n",
    "\n",
    "    for pca_value in pca_values:\n",
    "        for pi_value in pi_values:\n",
    "            # Generative models\n",
    "            gen_kfold(D, L, K, pca_value, pi_value)\n",
    "\n",
    "            # Discriminative models\n",
    "            for z_value in z_values:\n",
    "                for lambda_value in lambda_values:\n",
    "                    dis_kfold(D, L, K, pca_value, z_value, pi_value, lambda_value, z_value)\n",
    "        \n",
    "        # Support Vector Machines\n",
    "        for z_value in z_values:\n",
    "            for C_value in C_values:\n",
    "                for gamma_value in gamma_values:\n",
    "                    svm_kfold(D, L, K, pca_value, z_value, C_value, gamma_value)\n",
    "        \n",
    "        # Gaussian Mixture Models\n",
    "        if pca_value!=8 and pca_value!=6:\n",
    "            for z_value in z_values:\n",
    "                for g0_value in gmm_values:\n",
    "                    for g1_value in gmm_values:\n",
    "                        if g0_value>g1_value:\n",
    "                            gmm_kfold(D, L, K, pca_value, z_value, g0_value, g1_value)\n",
    "\n",
    "\n",
    "# ----- Sort training results with respect to minDCF -----\n",
    "def sort_training_results ():\n",
    "    # Sort results basing on minDCF\n",
    "    with open(\"../output/Training/Results.csv\", \"r\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        next(reader)\n",
    "        lines = list(reader)\n",
    "    sorted_lines = sorted(lines, key=lambda x: float(x[\"minDCF\"]), reverse=False)\n",
    "    \n",
    "    # Store ranking on a new CSV file\n",
    "    fieldnames = [\"Model\", \"PCA\", \"Z Norm\", \"pi\", \"lambda\", \"C\", \"gamma\", \"G0\", \"G1\", \"Accuracy\", \"Error rate\", \"minDCF\"]\n",
    "    with open(\"../output/Training/SortedResults.csv\", \"w+\") as file:\n",
    "        file.write(\"Model,PCA,ZNorm,pi,lambda,C,gamma,G0,G1,Accuracy,Error rate,minDCF\\n\")\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writerows(sorted_lines)\n",
    "    \n",
    "    best_models = sorted_lines[:3]\n",
    "    for model in best_models:\n",
    "        print(\"%s \\n\" % model)\n",
    "\n",
    "                        \n",
    "# ----- Select the three best models and train them -----\n",
    "def train_top3_models (D, L):\n",
    "    train_model(D, L)\n",
    "    sort_training_results()\n",
    "\n",
    "    # ---- Quadratic Logistic Regression ----\n",
    "    # --- PCA=8, ZNorm, pi=effective_prior, lambda=1e-2 ---\n",
    "    qlr_scores = train_qlr(D, L, 5, 8, 1, effective_prior, 1e-2)\n",
    "    \n",
    "    # ---- Polynomial Kernel SVM ----\n",
    "    # --- No PCA, No ZNorm, C=1, gamma=1e-3 ---\n",
    "    svm_scores = train_pol_svm(D, L, 5, 0, 0, 1e+0, 1e-3)\n",
    "\n",
    "    # ---- Diagonal Gaussian Mixture Model ----\n",
    "    # --- No PCA, No ZNorm, G0=8, G1=2 ---\n",
    "    gmm_scores = train_diagonal_gmm(D, L, 5, 0, 0, 8, 2)\n",
    "\n",
    "    return qlr_scores, svm_scores, gmm_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlr_scores, svm_scores, gmm_scores = train_top3_models(DTR, LTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Calibration and Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Bayes Error plot -----\n",
    "def bayes_error_plot (ll_ratios, labels, title):\n",
    "    eff_plo = numpy.linspace(-4, 4, 10)\n",
    "    eff_p = 1 / (1 + numpy.exp(-1*eff_plo))\n",
    "    actDCF = minDCF = []\n",
    "\n",
    "    for i in range(10):\n",
    "        actDCF.append(compute_actual_DCF(eff_p[i], 1, 1, numpy.hstack(ll_ratios), numpy.hstack(labels), True))\n",
    "        minDCF.append(compute_min_DCF(eff_p[i], 1, 1, numpy.hstack(ll_ratios), numpy.hstack(labels)))\n",
    "\n",
    "    matplotlib.pyplot.plot(eff_plo, actDCF, label='Actual DCF', color=\"b\")\n",
    "    matplotlib.pyplot.plot(eff_plo, minDCF, label='Min DCF', color=\"r\", linestyle=\"--\")\n",
    "    matplotlib.pyplot.xlabel(\"Effective prior log-odds\") \n",
    "    matplotlib.pyplot.ylabel(\"DCF value\")\n",
    "    matplotlib.pyplot.legend([\"act DCF\", \"min DCF\"])\n",
    "    matplotlib.pyplot.title(\"Bayes Error Plot\"+ title)\n",
    "    matplotlib.pyplot.ylim([0, 1])\n",
    "    matplotlib.pyplot.xlim([-4, 4])\n",
    "    matplotlib.pyplot.savefig(\"output/Calibration_Fusion/\" + title+ '.png')\n",
    "\n",
    "\n",
    "# ----- Score calibration -----\n",
    "def score_calibration (DTE, v, pi_value=effective_prior):\n",
    "    s = numpy.empty((DTE.shape[1]))\n",
    "    w,b = v[0:-1], v[-1]\n",
    "    for i in range(DTE.shape[1]):\n",
    "        xt = DTE[:,i]\n",
    "        s[i] = b + numpy.dot(w.T, xt) - numpy.log((pi_value) / (1-pi_value))\n",
    "    return numpy.array(s)\n",
    "\n",
    "\n",
    "# ----- Class calibration -----\n",
    "def class_calibration (DTR, LTR, DTE, LTE, pi_value, lambda_value):\n",
    "    x0 = numpy.zeros(DTR.shape[0] + 1)\n",
    "    x = numerical_optimization(lr_obj_wrap(DTR, LTR, lambda_value, pi_value), x0, lr_compute_gradient(DTR, LTR, lambda_value, pi_value))\n",
    "\n",
    "    scores = score_calibration(DTE, x, pi_value)\n",
    "    predicted_labels = predict_labels(scores, 0)\n",
    "    wrong_predictions = count_mispredictions(predicted_labels, LTE)\n",
    "\n",
    "    return wrong_predictions, scores\n",
    "\n",
    "\n",
    "# ----- Calibration function -----\n",
    "def compute_calibration (D, L, K, pi_value, lambda_value):\n",
    "    N = int(D.shape[1]/K)\n",
    "    \n",
    "    wrong_predictions = 0\n",
    "    numpy.random.seed(1)\n",
    "    ll_ratios = []\n",
    "    labels = []\n",
    "    indexes = numpy.random.permutation(D.shape[1])\n",
    "\n",
    "    for i in range(K):\n",
    "        # Select which subset to use for evaluation\n",
    "        idxTest = indexes[i*N:(i+1)*N]\n",
    "        if i>0: idxTrainLeft = indexes[0:i*N]\n",
    "        elif (i+1)<K: idxTrainRight = indexes[(i+1)*N:]\n",
    "        if i==0: idxTrain = idxTrainRight\n",
    "        elif (i+1)==K: idxTrain = idxTrainLeft\n",
    "        else: idxTrain = numpy.hstack([idxTrainLeft, idxTrainRight])\n",
    "        DTR,LTR = D[:,idxTrain], L[idxTrain]\n",
    "        DTE,LTE = D[:,idxTest], L[idxTest]\n",
    "\n",
    "        # Apply classifier\n",
    "        wrong, scores = class_calibration(DTR, LTR, DTE, LTE, pi_value, lambda_value)\n",
    "        wrong_predictions += wrong\n",
    "        ll_ratios.append(scores)\n",
    "        labels.append(LTE)\n",
    "\n",
    "    return numpy.hstack(ll_ratios)\n",
    "\n",
    "\n",
    "# ----- Calibrate scores -----\n",
    "def calibrate_scores (scores, labels, model_title):\n",
    "    scores = numpy.hstack(scores)\n",
    "\n",
    "    numpy.random.seed(100)\n",
    "    indexes = numpy.random.permutation(scores.shape[0])\n",
    "    sc = numpy.zeros((1, scores.shape[0]))\n",
    "    lab = numpy.zeros((labels.size,))\n",
    "    i = 0\n",
    "    for ind in indexes:\n",
    "        sc[0,i] = scores[ind]\n",
    "        lab[i] = labels[ind]\n",
    "        i += 1\n",
    "\n",
    "    calibrated = compute_calibration(sc, lab, 2, effective_prior, 1e-4)\n",
    "    \n",
    "    bayes_error_plot(scores, labels, model_title+'_uncalibrated')\n",
    "    bayes_error_plot(numpy.hstack(calibrated), lab, model_title+'_calibrated')\n",
    "\n",
    "    return calibrated\n",
    "\n",
    "\n",
    "# ----- Fuse two models -----\n",
    "def fuse_models (model1, scores1, model2, scores2, labels):\n",
    "    s = [ numpy.hstack(scores1), numpy.hstack(scores2) ]\n",
    "    s = numpy.vstack(s)\n",
    "    numpy.random.seed(5)\n",
    "    indexes = numpy.random.permutation(s.shape[1])\n",
    "    sc = numpy.zeros((2, s.shape[1]))\n",
    "    lab = numpy.zeros((labels.size,))\n",
    "    i = 0\n",
    "    for ind in indexes:\n",
    "        sc[:,i] = s[:,ind]\n",
    "        lab[i] = labels[ind]\n",
    "        i += 1\n",
    "\n",
    "    calibrated = compute_calibration(sc, lab, 2, effective_prior, 1e-3)\n",
    "    actualDCF = compute_actual_DCF(pi_t, Cfn, Cfp, numpy.hstack(calibrated), numpy.hstack(lab), False)\n",
    "\n",
    "    bayes_error_plot(numpy.hstack(calibrated), numpy.hstack(lab), model1+'_'+model2)\n",
    "\n",
    "    return calibrated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Calibrate scores -----\n",
    "def scores_calibration (L, qlr_scores, svm_scores, gmm_scores):\n",
    "    qlr_scores = calibrate_scores(qlr_scores, L, 'QLR')\n",
    "    svm_scores = numpy.hstack(numpy.array([]),numpy.array([]))#calibrate_scores(svm_scores, L, 'SVM')\n",
    "    gmm_scores = numpy.hstack(numpy.array([]),numpy.array([]))#calibrate_scores(gmm_scores, L, 'GMM')\n",
    "\n",
    "    return qlr_scores, svm_scores, gmm_scores\n",
    "\n",
    "\n",
    "# ----- Fuse top3 models -----\n",
    "def models_fusion (L, qlr_scores, svm_scores, gmm_scores):\n",
    "    qlr_svm_scores = fuse_models('QLR', qlr_scores, 'SVM', svm_scores, L)\n",
    "    qlr_gmm_scores = fuse_models('QLR', qlr_scores, 'GMM', gmm_scores, L)\n",
    "    svm_gmm_scores = fuse_models('SVM', svm_scores, 'GMM', gmm_scores, L)\n",
    "    \n",
    "    return qlr_svm_scores, qlr_gmm_scores, svm_gmm_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run calibration and fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlr_scores_cal, svm_scores_cal, gmm_scores_cal = scores_calibration(LTR, qlr_scores, svm_scores, gmm_scores)\n",
    "qlr_svm_scores, qlr_gmm_scores, svm_gmm_scores = models_fusion(LTR, qlr_scores_cal, svm_scores_cal, gmm_scores_cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Evaluate model -----\n",
    "def model_evaluation (DTR, LTR, DTE, LTE):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(DTR, LTR, DTE, LTE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
